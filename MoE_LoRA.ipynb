{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xN7LCKczCb8b",
        "DqzkdVE4xQ1l",
        "OEP_9k6PxT8h",
        "5ddgousA14PX",
        "8fhBJH-B3Vw-",
        "rqik4Tyqxtd7"
      ],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate peft\n",
        "!pip install tqdm\n",
        "!pip install rdkit\n",
        "!pip install PyTDC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd4H9dTG48Wh",
        "outputId": "132b3958-e687-4f91-d56f-a19b80b24dc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: PyTDC in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (from PyTDC) (2022.9.5)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (from PyTDC) (0.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PyTDC) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from PyTDC) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from PyTDC) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from PyTDC) (1.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from PyTDC) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from PyTDC) (2.31.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from PyTDC) (0.20.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.10/dist-packages (from PyTDC) (0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->PyTDC) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->PyTDC) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->PyTDC) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->PyTDC) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->PyTDC) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PyTDC) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PyTDC) (2023.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->PyTDC) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->PyTDC) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->PyTDC) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->PyTDC) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->PyTDC) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->PyTDC) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->PyTDC) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->PyTDC) (3.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->PyTDC) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->PyTDC) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->PyTDC) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->PyTDC) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->PyTDC) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->PyTDC) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->PyTDC) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Wrapper Classes"
      ],
      "metadata": {
        "id": "xN7LCKczCb8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0BFjH2kzzPvw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MoELoraModel(nn.Module):\n",
        "  def __init__(self, model, routing_network, config, num_experts):\n",
        "    super().__init__()\n",
        "    self.num_experts = num_experts\n",
        "    self.experts_model = LoraModel(model, config, \"default\")\n",
        "    self.experts_list = []\n",
        "\n",
        "    for i in range(num_experts):\n",
        "      self.experts_model.add_weighted_adapter([\"default\"],[1],f\"expert_{i}\")\n",
        "      self.experts_list.append(f\"expert_{i}\")\n",
        "\n",
        "    self.routing_network = routing_network\n",
        "\n",
        "  def forward(self, inputs_dict):\n",
        "    model_inputs = inputs_dict[\"model_inputs\"]\n",
        "    routing_network_inputs = inputs_dict[\"routing_network_inputs\"]\n",
        "    self.route(**routing_network_inputs)\n",
        "    return self.experts_model(**model_inputs)\n",
        "\n",
        "  def route(self, **inputs):\n",
        "    logits = self.routing_network(**inputs)  # logits should be shape (num_experts, )\n",
        "    expert_probabilities = self.topk_with_softmax(logits)\n",
        "    chosen_expert = torch.multinomial(expert_probabilities, 1)\n",
        "    self.choose_expert(chosen_expert.item())\n",
        "\n",
        "  def topk_with_softmax(self, logits):\n",
        "    values, indices = torch.topk(logits,2)\n",
        "    ret = torch.zeros(self.num_experts,device=next(self.parameters()).device)\n",
        "    values = values/torch.norm(values)  # todo: probably remove\n",
        "    expert_weights = torch.softmax(values.float(), dim=1)\n",
        "    ret[indices] = expert_weights\n",
        "    return ret\n",
        "\n",
        "  def choose_expert(self,expert_num):\n",
        "    self.experts_model.disable_adapter_layers()\n",
        "    self.experts_model.set_adapter(f\"expert_{expert_num}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RoutingNetworkFromTransformer(nn.Module):\n",
        "  def __init__(self, model, num_experts, embedding_dim=384):\n",
        "    super().__init__()\n",
        "    self.num_experts = num_experts\n",
        "    self.last_layer = nn.Sequential(nn.Linear(embedding_dim, num_experts), nn.Softmax())\n",
        "    self.model = model\n",
        "\n",
        "  def forward(self, **inputs):\n",
        "    outputs = self.model(**inputs)\n",
        "\n",
        "    # Extract the hidden states\n",
        "    if hasattr(outputs, \"last_hidden_state\"): # Depends on the pretrained model backbone\n",
        "      hidden_states = outputs.last_hidden_state\n",
        "    else:\n",
        "      hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "    # Aggregate hidden states to get a single vector representation (e.g., mean pooling)\n",
        "    embeddings = torch.mean(hidden_states, dim=1)\n",
        "    return self.last_layer(embeddings)"
      ],
      "metadata": {
        "id": "aPILWKPafzFQ"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pretrained Drug Model"
      ],
      "metadata": {
        "id": "DqzkdVE4xQ1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import torch\n",
        "\n",
        "drug_tokenizer = RobertaTokenizer.from_pretrained(\"gokceuludogan/ChemBERTaLM\")\n",
        "pretrained_drug_model = RobertaModel.from_pretrained(\"gokceuludogan/ChemBERTaLM\", output_hidden_states=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV20rP-wxQj0",
        "outputId": "2adf319c-018d-4504-c8d3-b602eaaaa16f"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at gokceuludogan/ChemBERTaLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-trained Target Model"
      ],
      "metadata": {
        "id": "OEP_9k6PxT8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig,AutoModelForSequenceClassification, AutoModel, AutoTokenizer\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", output_hidden_states=True)\n",
        "target_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "pretrained_target_model = AutoModelForSequenceClassification.from_config(config)\n",
        "num_lora_experts = 8"
      ],
      "metadata": {
        "id": "uVfhOUoD4vIa"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-Trained Routing Network Backbone"
      ],
      "metadata": {
        "id": "5ddgousA14PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "routing_network_backbone = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
        "routing_network_tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
        "\n",
        "routing_network = RoutingNetworkFromTransformer(routing_network_backbone, num_lora_experts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIXn4AzE17Ng",
        "outputId": "ef2e8ba9-d67c-4549-8a28-b0c4dda6dcf1"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Proposed Model"
      ],
      "metadata": {
        "id": "hjHu4SpJ2D4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraModel, LoraConfig\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=[\"query\",\"value\"],\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "rjjc2FTmkFiZ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_router = RoutingNetworkFromTransformer(pretrained_drug_model, num_lora_experts, embedding_dim=768)\n",
        "target_router = RoutingNetworkFromTransformer(pretrained_target_model, num_lora_experts, embedding_dim=1280)"
      ],
      "metadata": {
        "id": "G3nD4EXbHN_o"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_model = MoELoraModel(pretrained_drug_model, drug_router, lora_config, num_lora_experts)\n",
        "target_model = MoELoraModel(pretrained_target_model, target_router, lora_config, num_lora_experts)\n",
        "regressor = nn.Sequential(nn.Linear(2048,128),nn.ReLU(),nn.Linear(128,1))"
      ],
      "metadata": {
        "id": "0EdkbdgO4oKW"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in model.experts_model.classifier.parameters():\n",
        "#   param.requires_grad = True"
      ],
      "metadata": {
        "id": "jOPk_7_VFsUU"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Optimizers"
      ],
      "metadata": {
        "id": "8fhBJH-B3Vw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_parameters = list(drug_model.parameters()) + list(target_model.parameters()) + list(regressor.parameters())\n",
        "optimizer = torch.optim.AdamW(combined_parameters, lr=5e-5)"
      ],
      "metadata": {
        "id": "RvA9apiQ3Xeb"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Helper Functions"
      ],
      "metadata": {
        "id": "rqik4Tyqxtd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(tokenizer,model, input,device):\n",
        "    # Encode the SMILES sequence\n",
        "    encoded_input = tokenizer(input, return_tensors=\"pt\").to(device)\n",
        "    # routing_network_inputs = router_tokenizer(input, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    inputs_dict = {\n",
        "        \"model_inputs\" : encoded_input,\n",
        "        \"routing_network_inputs\": encoded_input\n",
        "    }\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs_dict)\n",
        "\n",
        "    # Extract the hidden states\n",
        "    if hasattr(outputs, \"last_hidden_state\"): # Depends on the pretrained model backbone\n",
        "      hidden_states = outputs.last_hidden_state\n",
        "    else:\n",
        "      hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "    # Aggregate hidden states to get a single vector representation (e.g., mean pooling)\n",
        "    embeddings = torch.mean(hidden_states, dim=1)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "TBrkVqlL1tLY"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "TMnF2nHqFMik"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "O5qUySNt5r2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(\n",
        "    drug_model,\n",
        "    target_model,\n",
        "    regressor,\n",
        "    drug_tokenizer,\n",
        "    target_tokenizer,\n",
        "    router_tokenizer,\n",
        "    optimizer,\n",
        "    data_loader,\n",
        "    get_embeddings_fn,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    num_epochs\n",
        "):\n",
        "  for epoch in range(num_epochs):\n",
        "    for input in tqdm(data_loader, desc=f\"Training epoch {epoch}\"):\n",
        "      drug_smiles = input['drug']\n",
        "      target_seq = input['target']\n",
        "      target_affinity = torch.tensor(input['affinity'],dtype=torch.float).to(device)\n",
        "\n",
        "      drug_embeddings = get_embeddings_fn(drug_tokenizer,drug_model,drug_smiles,device)\n",
        "      target_embeddings = get_embeddings_fn(target_tokenizer,target_model,target_seq,device)\n",
        "\n",
        "      all_embeds = torch.cat((drug_embeddings, target_embeddings), dim=1)\n",
        "\n",
        "      predicted_affinity = regressor(all_embeds)\n",
        "\n",
        "      loss = loss_fn(predicted_affinity,target_affinity)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "8VBDRppAAgtI"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tdc.multi_pred import DTI\n",
        "data = DTI(name = 'DAVIS')\n",
        "split = data.get_split()"
      ],
      "metadata": {
        "id": "Qv__ORHj5tJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5752e3f-86cf-4779-9089-28080a4c30cb"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found local copy...\n",
            "Loading...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DavisDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        # Assuming data is a dictionary with 'train', 'valid', 'test' splits\n",
        "        # Concatenate training, validation, and test sets if needed\n",
        "        # Or you can adjust the code to use only one of the splits\n",
        "        self.data = data['train']\n",
        "        self.drug = self.data['Drug']\n",
        "        self.target = self.data['Target']\n",
        "        self.affinity = self.data['Y']\n",
        "\n",
        "        # Here, additional preprocessing can be done (e.g., tokenization)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.affinity)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"drug\": self.drug[idx],\n",
        "            \"target\": self.target[idx],\n",
        "            \"affinity\": torch.tensor(self.affinity[idx], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "gpIw0dcY7HJE"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davis_dataset = DavisDataset(split)\n",
        "dataloader = DataLoader(davis_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "gZJAomFtIy0e"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "drug_model.to(device)\n",
        "target_model.to(device)\n",
        "regressor.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMKvZwmDCIJU",
        "outputId": "ad3e3421-af64-4b96-994e-b1978c07dfd0"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2048, out_features=128, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    drug_model,\n",
        "    target_model,\n",
        "    regressor,\n",
        "    drug_tokenizer,\n",
        "    target_tokenizer,\n",
        "    routing_network_tokenizer,\n",
        "    optimizer,\n",
        "    dataloader,\n",
        "    get_embeddings,\n",
        "    mse_loss_fn,\n",
        "    device,\n",
        "    5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgghOFOIJKFe",
        "outputId": "7da9d073-fbeb-47dd-8b67-d5e1cef6030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch 0:   1%|          | 206/18041 [00:59<1:18:27,  3.79it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device of a parameter after calling .to(device)\n",
        "print(next(drug_model.parameters()).device)  # Should output 'cuda' if a GPU is available, otherwise 'cpu'"
      ],
      "metadata": {
        "id": "oMIMg_IaD1uL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}